import asyncio
import json
from datetime import datetime
from pathlib import Path
from typing import List

from news_analyzer.core.news_loader import NewsLoader
from news_analyzer.core.hotness_calculator import HotnessCalculator
from news_analyzer.core.llm_client import LLMClient
from news_analyzer.core.entity_extractor import EntityExtractor
from news_analyzer.core.timeline_builder import TimelineBuilder
from news_analyzer.models.output_models import RadarOutput, BatchRadarOutput
from news_analyzer.config.settings import TOP_NEWS_COUNT, HOT_NEWS_THRESHOLD
from news_analyzer.utils.cache import CacheManager


class RadarNewsAnalyzer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π RADAR"""

    def __init__(self):
        self.news_loader = NewsLoader()
        self.hotness_calculator = HotnessCalculator()
        self.llm_client = LLMClient()
        self.entity_extractor = EntityExtractor(self.llm_client)
        self.timeline_builder = TimelineBuilder(self.llm_client)
        self.cache = CacheManager()

    async def analyze_news_file(self, file_path: str) -> BatchRadarOutput:
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ JSON —Ñ–∞–π–ª–∞"""

        print(f"üìÇ –ó–∞–≥—Ä—É–∂–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ {file_path}")

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è
        news_data = self.news_loader.load_json(file_path)
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(news_data.news)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        # 2. –†–∞—Å—á–µ—Ç –≥–æ—Ä—è—á–µ—Å—Ç–∏
        print("üî• –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≥–æ—Ä—è—á–Ω–æ—Å—Ç—å...")
        hot_news = []

        for news_item in news_data.news:
            hotness_score = await self.hotness_calculator.calculate_hotness(news_item)

            if hotness_score.total >= HOT_NEWS_THRESHOLD:
                hot_news.append((news_item, hotness_score))
                print(f"üåü –ì–æ—Ä—è—á–∞—è –Ω–æ–≤–æ—Å—Ç—å ({hotness_score.total:.3f}): {news_item.title[:50]}...")

        # 3. –¢–æ–ø-N —Å–∞–º—ã—Ö –≥–æ—Ä—è—á–∏—Ö
        hot_news.sort(key=lambda x: x.total, reverse=True)
        top_news = hot_news[:TOP_NEWS_COUNT]

        print(f"üî• –ù–∞–π–¥–µ–Ω–æ {len(hot_news)} –≥–æ—Ä—è—á–∏—Ö, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø-{len(top_news)}")

        # 4. –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ duplicate_group
        news_groups = self._group_news_by_dedup(top_news)

        # 5. LLM –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        radar_outputs = []

        for group_id, group_news in news_groups.items():
            print(f"ü§ñ –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≥—Ä—É–ø–ø—É: {group_id}")

            try:
                radar_output = await self._analyze_news_group(group_news)
                radar_outputs.append(radar_output)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≥—Ä—É–ø–ø—ã {group_id}: {e}")
                continue

        # 6. –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = BatchRadarOutput(
            timestamp=datetime.now(),
            top_events=radar_outputs,
            total_processed=len(news_data.news),
            hot_news_count=len(hot_news),
            processing_stats={
                "avg_hotness": sum(s.total for _, s in hot_news) / len(hot_news) if hot_news else 0,
                "groups_analyzed": len(news_groups),
                "api_calls_made": self.llm_client.api_calls_count
            }
        )

        return result

    def _group_news_by_dedup(self, hot_news: List) -> dict:
        """–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ duplicate_group"""
        groups = {}

        for news_item, hotness_score in hot_news:
            group_id = news_item.duplicate_group

            if group_id not in groups:
                groups[group_id] = []

            groups[group_id].append((news_item, hotness_score))

        return groups

    async def _analyze_news_group(self, group_news: List) -> RadarOutput:
        """–ê–Ω–∞–ª–∏–∑ –≥—Ä—É–ø–ø—ã —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""

        # –û—Å–Ω–æ–≤–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å - —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≥–æ—Ä—è—á–Ω–æ—Å—Ç—å—é
        main_news, main_hotness = max(group_news, key=lambda x: x.total)
        all_news = [item for item in group_news]

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entities = await self.entity_extractor.extract_entities(main_news.content)

        # Timeline
        timeline = await self.timeline_builder.build_timeline(all_news)

        # Why now –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
        why_now = await self.llm_client.generate_why_now(
            main_news.content,
            f"Hotness: {main_hotness.total:.2f}, Sources: {len(all_news)}"
        )

        # –ß–µ—Ä–Ω–æ–≤–∏–∫ —Å—Ç–∞—Ç—å–∏
        draft = await self.llm_client.generate_draft(
            main_news.content, entities, why_now
        )

        # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–±—ã—Ç–∏—è
        headline = await self.llm_client.generate_headline(main_news.content, entities)

        # –°–ø–∏—Å–æ–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        sources = list(set([news.url for news in all_news]))

        # –°–ø–∏—Å–æ–∫ —Å—É—â–Ω–æ—Å—Ç–µ–π
        entity_list = []
        if entities.companies:
            entity_list.extend([comp.get('name', '') for comp in entities.companies])
        if entities.countries:
            entity_list.extend(entities.countries)
        if entities.people:
            entity_list.extend(entities.people)

        return RadarOutput(
            headline=headline,
            hotness=main_hotness.total,
            why_now=why_now,
            entities=entity_list[:20],
            sources=sources[:5],
            timeline=timeline,
            draft=draft,
            dedup_group=main_news.duplicate_group
        )

    async def save_results(self, results: BatchRadarOutput, output_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""

        output_data = results.model_dump()

        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è datetime –¥–ª—è JSON
        output_data['timestamp'] = results.timestamp.isoformat()

        for event in output_data['top_events']:
            for timeline_item in event['timeline']:
                timeline_item['time'] = timeline_item['time'].isoformat()

        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)

        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {output_path}")


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""

    analyzer = RadarNewsAnalyzer()

    # –ü—É—Ç–∏ —Ñ–∞–π–ª–æ–≤
    input_file = "data/sample_news.json"
    output_file = f"data/processed/radar_output_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
    Path("data/processed").mkdir(parents=True, exist_ok=True)
    Path("data/cache").mkdir(parents=True, exist_ok=True)

    try:
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
        if not Path(input_file).exists():
            print("üìù –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
            analyzer.news_loader.create_sample_data(input_file)

        # –ê–Ω–∞–ª–∏–∑
        results = await analyzer.analyze_news_file(input_file)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        await analyzer.save_results(results, output_file)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"–í—Å–µ–≥–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {results.total_processed}")
        print(f"–ì–æ—Ä—è—á–∏—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {results.hot_news_count}")
        print(f"–¢–æ–ø —Å–æ–±—ã—Ç–∏–π: {len(results.top_events)}")
        print(f"API –≤—ã–∑–æ–≤–æ–≤: {results.processing_stats['api_calls_made']}")
        print(f"–°—Ä–µ–¥–Ω—è—è –≥–æ—Ä—è—á–Ω–æ—Å—Ç—å: {results.processing_stats['avg_hotness']:.3f}")

        # –¢–æ–ø —Å–æ–±—ã—Ç–∏—è
        print("\nüî• –¢–û–ü –°–û–ë–´–¢–ò–Ø:")
        for i, event in enumerate(results.top_events[:3], 1):
            print(f"{i}. {event.headline} (hotness: {event.hotness:.3f})")
            print(f"   Why now: {event.why_now}")
            print()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())