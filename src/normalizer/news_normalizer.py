import os
import json
import hashlib
import time
from datetime import datetime, timezone
from uuid import uuid4
import requests
from bs4 import BeautifulSoup
from newspaper import Article
from langdetect import detect
from collections import Counter
import re

# === –ü–∞–ø–∫–∏ ===
BASE_DIR = os.path.dirname(os.path.dirname(__file__))
PARSER_DIR = os.path.join(BASE_DIR, "parser")
DATA_DIR = os.path.join(BASE_DIR, "data")
NORMALIZED_DIR = os.path.join(DATA_DIR, "normalized")

os.makedirs(PARSER_DIR, exist_ok=True)
os.makedirs(NORMALIZED_DIR, exist_ok=True)

# === Scraper API (–¥–ª—è –æ–±—Ö–æ–¥–∞ 401/403) ===
SCRAPER_API_KEY = os.getenv("SCRAPER_API_KEY", "")
SCRAPER_API_URL = f"https://api.scraperapi.com?api_key={SCRAPER_API_KEY}&url=" if SCRAPER_API_KEY else None

# === –£—Ä–æ–≤–Ω–∏ –¥–æ–≤–µ—Ä–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º ===
SOURCE_CREDIBILITY = {
    "newsdata": 1,
    "marketaux": 2,
    "newsapi": 3,
    "polygon": 4,
    "finnhub": 5
}


# === –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò ===

def generate_hash(text: str) -> str:
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def extract_keywords(text: str, top_n: int = 10) -> list:
    """–ü—Ä–æ—Å—Ç–æ–µ –≤—ã–¥–µ–ª–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –ø–æ —á–∞—Å—Ç–æ—Ç–µ"""
    text = re.sub(r"[^a-zA-Z–∞-—è–ê-–Ø\s]", "", text)
    words = text.lower().split()
    stop_words = {"the", "and", "for", "that", "this", "with", "from", "was", "were", "will", "have", "has", "are"}
    words = [w for w in words if len(w) > 3 and w not in stop_words]
    return [w for w, _ in Counter(words).most_common(top_n)]


def is_truncated_text(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —É—Å–µ—á—ë–Ω –ª–∏ —Ç–µ–∫—Å—Ç (–µ—Å—Ç—å –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –∏–ª–∏ —Ñ—Ä–∞–∑–∞ [+123 chars])"""
    if not text:
        return True
    if len(text.strip()) < 200:
        return True
    if "..." in text.strip()[-10:]:
        return True
    if re.search(r"\[\+\d+\schars\]", text):
        return True
    return False


def enrich_from_url(article: dict) -> dict:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ URL –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–æ–ª—è"""
    url = article.get("url")
    if not url:
        return article

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/127.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://www.google.com/",
        "Connection": "keep-alive",
    }

    # === 1Ô∏è‚É£ Newspaper3k ===
    try:
        art = Article(url)
        art.download()
        art.parse()
        if art.text and len(art.text) > 200:
            article["content"] = art.text.strip()
            if art.authors:
                article["author"] = ", ".join(art.authors)
            if art.publish_date:
                article["published_at"] = art.publish_date.isoformat()

            if not article.get("language"):
                try:
                    article["language"] = detect(art.text[:500])
                except:
                    pass

            article["keywords"] = extract_keywords(art.text)
            print(f"‚úÖ Newspaper3k –∏–∑–≤–ª–µ–∫ {len(art.text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {url[:60]}...")
            return article
    except Exception:
        pass

    # === 2Ô∏è‚É£ Requests + BeautifulSoup ===
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            soup = BeautifulSoup(resp.text, "html.parser")
            paragraphs = [p.get_text() for p in soup.find_all("p")]
            text = "\n".join(paragraphs).strip()
            if text:
                article["content"] = text
                article["keywords"] = extract_keywords(text)
                if not article.get("language"):
                    try:
                        article["language"] = detect(text[:500])
                    except:
                        pass
                print(f"‚úÖ BeautifulSoup –∏–∑–≤–ª—ë–∫ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ {url[:60]}...")
                return article
        elif resp.status_code in (401, 403) and SCRAPER_API_URL:
            # === 3Ô∏è‚É£ ScraperAPI ===
            print(f"‚ö†Ô∏è 401 –¥–ª—è {url[:60]}... ‚Üí ScraperAPI")
            scraper_url = SCRAPER_API_URL + url
            r2 = requests.get(scraper_url, headers=headers, timeout=15)
            if r2.status_code == 200:
                soup = BeautifulSoup(r2.text, "html.parser")
                paragraphs = [p.get_text() for p in soup.find_all("p")]
                text = "\n".join(paragraphs).strip()
                if text:
                    article["content"] = text
                    article["keywords"] = extract_keywords(text)
                    if not article.get("language"):
                        try:
                            article["language"] = detect(text[:500])
                        except:
                            pass
                    print(f"‚úÖ ScraperAPI —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–∏–ª —Ç–µ–∫—Å—Ç ({len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return article
            else:
                print(f"‚ö†Ô∏è ScraperAPI –Ω–µ –ø–æ–º–æ–≥ ({r2.status_code}) –¥–ª—è {url[:60]}...")
        else:
            print(f"‚ö†Ô∏è HTTP {resp.status_code} –¥–ª—è {url[:60]}...")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url[:60]}...: {e}")

    # === 4Ô∏è‚É£ fallback ‚Äî —Ö–æ—Ç—è –±—ã description ===
    if not article.get("content") and article.get("description"):
        article["content"] = article["description"]

    return article


# === –û–°–ù–û–í–ù–ê–Ø –ù–û–†–ú–ê–õ–ò–ó–ê–¶–ò–Ø ===

def normalize_article(article: dict, source: str) -> dict:
    now = datetime.now(timezone.utc).isoformat()
    uid = article.get("id") or generate_hash(article.get("url", str(uuid4())))
    content = article.get("content") or article.get("description") or None

    normalized = {
        "id": uid,
        "title": article.get("title") or article.get("headline") or None,
        "content": content,
        "url": article.get("url") or None,
        "source": source,
        "author": article.get("author") or None,
        "language": article.get("language") or None,
        "country": article.get("country") or None,
        "category": article.get("category") or None,
        "tags": article.get("tags") or [],
        "tickers": article.get("tickers") or [],
        "keywords": article.get("keywords") or [],
        "entities": article.get("entities") or [],
        "sentiment": None,
        "relevance": None,
        "hotness": None,
        "source_credibility": SOURCE_CREDIBILITY.get(source.lower(), None),
        "credibility_score": None,
        "is_duplicate": -1,
        "collected_at": now,
        "created_at": None,
        "updated_at": None,
        "published_at": article.get("published_at") or None,
    }

    # üöÄ –ù–æ–≤—ã–π —Ñ–∏–ª—å—Ç—Ä: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –æ–±—Ä–µ–∑–∞–Ω –ª–∏ —Ç–µ–∫—Å—Ç
    if is_truncated_text(normalized["content"]):
        normalized = enrich_from_url(normalized)

    return normalized


def load_articles(file_path: str) -> list:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç–∞—Ç—å–∏ –∏–∑ JSON"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict):
            if "filtered_articles" in data:
                return data["filtered_articles"]
            if "raw_data" in data and isinstance(data["raw_data"], dict):
                return data["raw_data"].get("articles", [])
        return []
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {file_path}: {e}")
        return []


def normalize_file(input_path: str, output_path: str, source: str):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω JSON-—Ñ–∞–π–ª"""
    articles = load_articles(input_path)
    normalized = [normalize_article(a, source) for a in articles]
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(normalized, f, ensure_ascii=False, indent=2)
    print(f"üíæ [{source}] {len(normalized)} –Ω–æ–≤–æ—Å—Ç–µ–π ‚Üí {output_path}")


def main(cycles: int = 2, delay: int = 5):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª–∏—á–µ—Å–∫—É—é –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é"""
    sources = {
        "newsapi": ("news_newsapi_latest.json", "newsapi.json"),
        "polygon": ("news_polygon_latest.json", "polygon.json"),
        "finnhub": ("news_finnhub_latest.json", "finnhub.json"),
        "marketaux": ("news_marketaux_latest.json", "marketaux.json"),
        "newsdata": ("news_newsdata_latest.json", "newsdata.json"),
    }

    for cycle in range(1, cycles + 1):
        print(f"\nüîÅ –¶–ò–ö–õ {cycle}/{cycles} ({datetime.now(timezone.utc).strftime('%H:%M:%S')})")
        for source, (input_name, output_name) in sources.items():
            input_path = os.path.join(PARSER_DIR, input_name)
            output_path = os.path.join(NORMALIZED_DIR, output_name)
            normalize_file(input_path, output_path, source)
        if cycle < cycles:
            print(f"‚è≥ –ü–∞—É–∑–∞ {delay} —Å–µ–∫...\n")
            time.sleep(delay)
    print("\n‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ data/normalized/")


if __name__ == "__main__":
    main()
